{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Head-Score NTK + Mixed RoPE Demo\n",
    "\n",
    "参照 `llada_generate.ipynb` 的扩散式解码流程，并结合 `mixed_rope_patch.py` 对特定注意力头应用 RoPE 放缩补丁。本 Notebook 直接使用本地 `modeling_llada.py` 与 head-score JSON，便于快速调试 per-head NTK scaling。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: 2\n",
      "Inside this process cuda:0 maps to physical GPU 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "TARGET_GPU = os.environ.get(\"TARGET_GPU\", \"2\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = TARGET_GPU\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(\"Inside this process cuda:0 maps to physical GPU\", TARGET_GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylong030/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Weights dir: /data/ylong030/huggingface/hub/models--relaxe-system-lab--UltraLLaDA/snapshots/ultrallada\n",
      "Head scores: ../head_score/llada-block-2500.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from configuration_llada import LLaDAConfig\n",
    "from modeling_llada import LLaDAModelLM\n",
    "from mixed_rope_patch import apply_mixed_rope_patch\n",
    "\n",
    "MODEL_PATH = Path(\"/data/ylong030/huggingface/hub/models--GSAI-ML--LLaDA-8B-Instruct/snapshots/08b83a6feb34df1a6011b80c3c00c7563e963b07\").expanduser()\n",
    "MODEL_PATH = Path(\"/data/ylong030/huggingface/hub/models--relaxe-system-lab--UltraLLaDA/snapshots/ultrallada\").expanduser()\n",
    "\n",
    "HEAD_SCORE_PATH = Path(\"../head_score/llada-block-2500.json\").expanduser()\n",
    "ROPE_SCALING_FACTOR: Optional[float] = 31\n",
    "HEAD_SCORE_TOP_K = 16\n",
    "HEAD_SCORE_THRESHOLD: Optional[float] = None  # 例如 0.5\n",
    "\n",
    "DTYPE = torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "GEN_STEPS = 64\n",
    "GEN_LENGTH = 64\n",
    "BLOCK_LENGTH = 64\n",
    "TEMPERATURE = 0.0\n",
    "CFG_SCALE = 0.0\n",
    "REMASKING = \"low_confidence\"  # or 'random'\n",
    "MASK_TOKEN_ID = 126336\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"The capital of France is\",\n",
    "    \"Explain why dynamic head-score NTK scaling can help long-context retrieval.\",\n",
    "]\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Weights dir: {MODEL_PATH}\")\n",
    "print(f\"Head scores: {HEAD_SCORE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_head_scores(path: Path) -> List[Tuple[Tuple[int, int], float]]:\n",
    "    text = path.read_text().strip()\n",
    "    if not text:\n",
    "        raise ValueError(f\"Empty head score file: {path}\")\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        with path.open() as f:\n",
    "            data = json.loads(f.readline())\n",
    "    scored = []\n",
    "    for key, values in data.items():\n",
    "        try:\n",
    "            layer_idx, head_idx = map(int, key.split('-'))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if isinstance(values, Sequence):\n",
    "            vals = [float(v) for v in values if v is not None]\n",
    "            if not vals:\n",
    "                continue\n",
    "            score = sum(vals) / len(vals)\n",
    "        else:\n",
    "            score = float(values)\n",
    "        scored.append(((layer_idx, head_idx), float(score)))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored\n",
    "\n",
    "def select_scaled_heads(\n",
    "    scored_heads: List[Tuple[Tuple[int, int], float]],\n",
    "    top_k: Optional[int],\n",
    "    threshold: Optional[float] = None,\n",
    ") -> Dict[int, set]:\n",
    "    selected: Dict[int, set] = {}\n",
    "    total = 0\n",
    "    for (layer_idx, head_idx), score in scored_heads:\n",
    "        if threshold is not None and score < threshold:\n",
    "            break\n",
    "        if top_k is not None and total >= top_k:\n",
    "            break\n",
    "        selected.setdefault(layer_idx, set()).add(head_idx)\n",
    "        total += 1\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 43.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading head scores...\n",
      "Selected 16 heads across 10 layers\n",
      "{18: {1}, 21: {12, 21}, 23: {9, 19, 13, 15}}\n",
      "================================================================================\n",
      "Applying Mixed RoPE Patch\n",
      "================================================================================\n",
      "Scaling factor: 32\n",
      "Layers affected: 10\n",
      "Total heads scaled: 16\n",
      "\n",
      "ModelConfig(d_model=4096, n_heads=32, n_kv_heads=32, n_layers=32, mlp_ratio=4, mlp_hidden_size=12288, activation_type='silu', block_type='llama', block_group_size=1, alibi=False, alibi_bias_max=8.0, rope=True, rope_full_precision=True, flash_attention=False, attention_dropout=0.0, multi_query_attention=None, attention_layer_norm=False, residual_dropout=0.0, embedding_dropout=0.0, input_emb_norm=False, layer_norm_type='rms', layer_norm_with_affine=True, rms_norm_eps=1e-05, attention_layer_norm_with_affine=True, max_sequence_length=4096, rope_theta=281000000.0, rope_scaling_factor=32, scaled_heads_dict=None, head_score_path=None, head_score_top_k=0, head_score_threshold=None, include_qkv_bias=False, include_bias=False, bias_for_layer_norm=False, scale_logits=False, vocab_size=126464, embedding_size=126464, weight_tying=False, eos_token_id=126081, pad_token_id=126081, mask_token_id=126336, init_device='cpu', init_fn='mitchell', init_std=0.02, init_cutoff_factor=None, precision='amp_bf16')\n",
      "Layer 15: Patched RoPE for heads [18]\n",
      "Layer 16: Patched RoPE for heads [25]\n",
      "Layer 17: Patched RoPE for heads [19, 22]\n",
      "Layer 18: Patched RoPE for heads [1]\n",
      "Layer 19: Patched RoPE for heads [1, 21]\n",
      "Layer 21: Patched RoPE for heads [12, 21]\n",
      "Layer 23: Patched RoPE for heads [9, 13, 15, 19]\n",
      "Layer 24: Patched RoPE for heads [3]\n",
      "Layer 25: Patched RoPE for heads [15]\n",
      "Layer 31: Patched RoPE for heads [11]\n",
      "\n",
      "================================================================================\n",
      "Patching Complete!\n",
      "================================================================================\n",
      "Model dtype: torch.bfloat16\n",
      "Configured rope scaling factor: 32\n",
      "Patched scaled heads stored on config: True\n"
     ]
    }
   ],
   "source": [
    "config = LLaDAConfig.from_pretrained(str(MODEL_PATH))\n",
    "config.use_cache = False  # diffusion decoding does not reuse KV cache\n",
    "if ROPE_SCALING_FACTOR is not None:\n",
    "    config.rope_scaling_factor = ROPE_SCALING_FACTOR\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH), trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LLaDAModelLM.from_pretrained(\n",
    "    str(MODEL_PATH),\n",
    "    config=config,\n",
    "    torch_dtype=DTYPE,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "scaled_heads_dict: Dict[int, set] = {}\n",
    "scored_heads: List[Tuple[Tuple[int, int], float]] = []\n",
    "if HEAD_SCORE_PATH.exists():\n",
    "    print(\"Loading head scores...\")\n",
    "    scored_heads = load_head_scores(HEAD_SCORE_PATH)\n",
    "    selection = select_scaled_heads(\n",
    "        scored_heads,\n",
    "        top_k=HEAD_SCORE_TOP_K,\n",
    "        threshold=HEAD_SCORE_THRESHOLD,\n",
    "    )\n",
    "    scaled_heads_dict.clear()\n",
    "    scaled_heads_dict.update(selection)\n",
    "    total_heads = sum(len(v) for v in scaled_heads_dict.values())\n",
    "    print(f\"Selected {total_heads} heads across {len(scaled_heads_dict)} layers\")\n",
    "    if scaled_heads_dict:\n",
    "        preview = dict(list(scaled_heads_dict.items())[:3])\n",
    "        pprint(preview)\n",
    "    apply_mixed_rope_patch(\n",
    "        model,\n",
    "        scaling_factor=ROPE_SCALING_FACTOR or 1.0,\n",
    "        scaled_heads_dict=scaled_heads_dict,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    print(f\"⚠️  head score file not found: {HEAD_SCORE_PATH}\")\n",
    "\n",
    "print(\"Model dtype:\", next(model.parameters()).dtype)\n",
    "print(\"Configured rope scaling factor:\", getattr(model.config, \"rope_scaling_factor\", \"n/a\"))\n",
    "print(\"Patched scaled heads stored on config:\", bool(getattr(model.config, \"scaled_heads_dict\", {})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def add_gumbel_noise(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits64 = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits64)\n",
    "    gumbel_noise = (-torch.log(noise)) ** temperature\n",
    "    return logits64.exp() / gumbel_noise\n",
    "\n",
    "def get_num_transfer_tokens(mask_index: torch.Tensor, steps: int) -> torch.Tensor:\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "    plan = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "    for row in range(mask_num.size(0)):\n",
    "        extra = int(remainder[row].item())\n",
    "        if extra > 0:\n",
    "            plan[row, :extra] += 1\n",
    "    return plan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def llada_decode(\n",
    "    model: LLaDAModelLM,\n",
    "    prompt_ids: torch.Tensor,\n",
    "    steps: int = GEN_STEPS,\n",
    "    gen_length: int = GEN_LENGTH,\n",
    "    block_length: int = BLOCK_LENGTH,\n",
    "    temperature: float = TEMPERATURE,\n",
    "    cfg_scale: float = CFG_SCALE,\n",
    "    remasking: str = REMASKING,\n",
    "    mask_id: int = MASK_TOKEN_ID,\n",
    ") -> torch.Tensor:\n",
    "    device = next(model.parameters()).device\n",
    "    prompt_ids = prompt_ids.to(device)\n",
    "    batch_size, prompt_len = prompt_ids.shape\n",
    "    total_len = prompt_len + gen_length\n",
    "    x = torch.full((batch_size, total_len), mask_id, dtype=torch.long, device=device)\n",
    "    x[:, :prompt_len] = prompt_ids\n",
    "    prompt_mask = (x != mask_id)\n",
    "\n",
    "    assert gen_length % block_length == 0, \"gen_length must be divisible by block_length\"\n",
    "    num_blocks = gen_length // block_length\n",
    "    assert steps % num_blocks == 0, \"steps must be divisible by (gen_length / block_length)\"\n",
    "    steps_per_block = steps // num_blocks\n",
    "\n",
    "    for block_idx in range(num_blocks):\n",
    "        block_start = prompt_len + block_idx * block_length\n",
    "        block_end = block_start + block_length\n",
    "        block_mask = (x[:, block_start:block_end] == mask_id)\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask, steps_per_block)\n",
    "\n",
    "        for step_idx in range(steps_per_block):\n",
    "            mask_index = (x == mask_id)\n",
    "            if cfg_scale > 0.0:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_mask] = mask_id\n",
    "                x_in = torch.cat([x, un_x], dim=0)\n",
    "                logits = model(input_ids=x_in, use_cache=False).logits\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(input_ids=x, use_cache=False).logits\n",
    "\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1)\n",
    "\n",
    "            if remasking == \"low_confidence\":\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                gather_index = x0.unsqueeze(-1)\n",
    "                x0_p = torch.squeeze(torch.gather(probs, dim=-1, index=gather_index), -1)\n",
    "            elif remasking == \"random\":\n",
    "                x0_p = torch.rand((batch_size, total_len), device=device)\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Unknown remasking strategy: {remasking}\")\n",
    "\n",
    "            x0_p[:, block_end:] = float('-inf')\n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, torch.full_like(x0_p, float('-inf')))\n",
    "\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool)\n",
    "            for row in range(batch_size):\n",
    "                quota = int(num_transfer_tokens[row, step_idx].item())\n",
    "                if quota <= 0:\n",
    "                    continue\n",
    "                quota = min(quota, confidence.shape[1])\n",
    "                _, indices = torch.topk(confidence[row], k=quota)\n",
    "                transfer_index[row, indices] = True\n",
    "            x[transfer_index] = x0[transfer_index]\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_prompt(\n",
    "    prompt: str,\n",
    "    steps: int = GEN_STEPS,\n",
    "    gen_length: int = GEN_LENGTH,\n",
    "    block_length: int = BLOCK_LENGTH,\n",
    "    temperature: float = TEMPERATURE,\n",
    "    cfg_scale: float = CFG_SCALE,\n",
    "    remasking: str = REMASKING,\n",
    ") -> str:\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output_ids = llada_decode(\n",
    "        model,\n",
    "        encoded[\"input_ids\"],\n",
    "        steps=steps,\n",
    "        gen_length=gen_length,\n",
    "        block_length=block_length,\n",
    "        temperature=temperature,\n",
    "        cfg_scale=cfg_scale,\n",
    "        remasking=remasking,\n",
    "    )\n",
    "    completion_ids = output_ids[:, encoded[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(completion_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "def run_batch(prompts: List[str], **kwargs) -> None:\n",
    "    for idx, prompt in enumerate(prompts, 1):\n",
    "        print(f\"Prompt {idx}: {prompt}\")\n",
    "        completion = decode_prompt(prompt, **kwargs)\n",
    "        print(completion if completion else \"[empty]\")\n",
    "        print(\"-\" * 72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a numbered list of words. In these words, some appear more often than others. Memorize the ones that appear most often.\n",
      "1. exotic\n"
     ]
    }
   ],
   "source": [
    "# 手动 prompt，可根据需要反复修改后运行本单元\n",
    "custom_prompt ='''I attended to all the ghastly\\nformalities, and the urbane undertaker proved that his staff were\\nafflicted--or blessed--with something of his own obsequious suavity.\\nEven the woman who performed the last offices for the dead remarked to\\nme, in a confidential, brother-professional way, when she had come out\\nfrom the death-chamber:--\\n\\n\\\"She makes a very beautiful corpse, sir. It's quite a privilege to\\nattend on her. It's not too much to say that she will do credit to our\\nestablishment!\\\"\\n\\nI noticed that Van Helsing never kept far away. This was possible from\\nthe disordered state of things in the household. There were no relatives\\nat hand; and as Arthur had to be back the next day to attend at his\\nfather's funeral, we were unable to notify any one who should have been\\nbidden. Under the circumstances, Van Helsing and I took it upon\\nourselves to examine papers, etc. He insisted upon looking over Lucy's\\npapers himself. I asked him why, for I feared that he, being a\\nforeigner, might not be quite aware of English legal requirements, and\\nso might in ignorance make some unnecessary trouble. He answered me:--\\n\\n\\\"I know; I know. You forget that I am a lawyer as well as a doctor. But\\nthis is not altogether for the law. You knew that, when you avoided the\\ncoroner. I have more than him to avoid. There may be papers more--such\\nas this.\\\"\\n\\nAs he spoke he took from his pocket-book the memorandum which had been\\nin Lucy's breast, and which she had torn in her sleep.\\n\\n\\\"When you find anything of the solicitor who is for the late Mrs.\\nWestenra, seal all her papers, and write him to-night. For me, I watch\\nhere in the room and in Miss Lucy's old room all night, and I myself\\nsearch for what may be. It is not well that her very thoughts go into\\nthe hands of strangers.\\\"\\n\\nI went on with my part of the work, and in another half hour had found\\nthe name and address of Mrs. Westenra's solicitor and had written to\\nhim. All the poor lady's papers were in order; explicit directions\\nregarding the place of burial were given. I had hardly sealed the\\nletter, when, to my surprise, Van Helsing walked into the room,\\nsaying:--\\n\\n\\\"Can I help you, friend John? I am free, and if I may, my service is to\\nyou.\\\"\\n\\n\\\"Have you got what you looked for?\\\" I asked, to which he replied:--\\n\\n\\\"I did not look for any specific thing. I only hoped to find, and find I\\nhave, all that there was--only some letters and a few memoranda, and a\\ndiary new begun. But I have them here, and we shall for the present say\\nnothing of them. I shall see that poor lad to-morrow evening, and, with\\nhis sanction, I shall use some.\\\"\\n\\nWhen we had finished the work in hand, he said to me:--\\n\\n\\\"And now, friend John, I think we may to bed. We want sleep, both you\\nand I, and rest to recuperate. To-morrow we shall have much to do, but\\nfor the to-night there is no need of us. Alas!\\\"\\n\\nBefore turning in we went to look at poor Lucy. The undertaker had\\ncertainly done his work well, for the room was turned into a small\\n_chapelle ardente_. There was a wilderness of beautiful white flowers,\\nand death was made as little repulsive as might be. The end of the\\nwinding-sheet was laid over the face; when the Professor bent over and\\nturned it gently back, we both started at the beauty before us, the tall\\nwax candles showing a sufficient light to note it well. All Lucy's\\nloveliness had come back to her in death, and the hours that had passed,\\ninstead of leaving traces of \\\"decay's effacing fingers,\\\" had but\\nrestored the beauty of life, till positively I could not believe my eyes\\nthat I was looking at a corpse.\\n\\nThe Professor looked sternly grave. He had not loved her as I had, and\\nthere was no need for tears in his eyes. He said to me: \\\"Remain till I\\nreturn,\\\" and left the room. He came back with a handful of wild garlic\\nfrom the box waiting in the hall, but which had not been opened, and\\nplaced the flowers amongst the others on and around the bed. Then he\\ntook from his neck, inside his collar, a little gold crucifix, and\\nplaced it over the mouth. He restored the sheet to its place, and we\\ncame away.\\n\\nI was undressing in my own room, when, with a premonitory tap at the\\ndoor, he entered, and at once began to speak:--\\n\\n\\\"To-morrow I want you to bring me, before night, a set of post-mortem\\nknives.\\\"\\n\\n\\\"Must we make an autopsy?\\\" I asked.\\n\\n\\\"Yes and no. I want to operate, but not as you think. Let me tell you\\nnow, but not a word to another. I want to cut off her head and take out\\nher heart. Ah! you a surgeon, and so shocked! You, whom I have seen with\\nno tremble of hand or heart, do operations of life and death that make\\nthe rest shudder. Oh, but I must not forget, my dear friend John, that\\nyou loved her; and I have not forgotten it, for it is I that shall\\noperate, and you must only help. I would like to do it to-night, but for\\nArthur I must not; he will be free after his father's funeral to-morrow,\\nand he will want to see her--to see _it_. Then, when she is coffined\\nready for the next day, you and I shall come when all sleep. We shall\\nunscrew the coffin-lid, and shall do our operation: and then replace\\nall, so that none know, save we alone.\\\"\\n\\n\\\"But why do it at all? The girl is dead. Why mutilate her poor body\\nwithout need? And if there is no necessity for a post-mortem and nothing\\nto gain by it--no good to her, to us, to science, to human\\nknowledge--why do it? Without such it is monstrous.\\\"\\n\\nFor answer he put his hand on my shoulder, and said, with infinite\\ntenderness:--\\n\\n\\\"Friend John, I pity your poor bleeding heart; and I love you the more\\nbecause it does so bleed. If I could, I would take on myself the burden\\nthat you do bear. But there are things that you know not, but that you\\nshall know, and bless me for knowing, though they are not pleasant\\nthings. John, my child, you have been my friend now many years, and yet\\ndid you ever know me to do any without good cause? I may err--I am but\\nman; but I believe in all I do. Was it not for these causes that you\\nsend for me when the great trouble came? Yes! Were you not amazed, nay\\nhorrified, when I would not let Arthur kiss his love--though she was\\ndying--and snatched him away by all my strength? Yes! And yet you saw\\nhow she thanked me, with her so beautiful dying eyes, her voice, too, so\\nweak, and she kiss my rough old hand and bless me? Yes! And did you not\\nhear me swear promise to her, that so she closed her eyes grateful? Yes!\\n\\n\\\"Well, I have good reason now for all I want to do. You have for many\\nyears trust me; you have believe me weeks past, when there be things so\\nstrange that you might have well doubt. Believe me yet a little, friend\\nJohn. If you trust me not, then I must tell what I think; and that is\\nnot perhaps well. And if I work--as work I shall, no matter trust or no\\ntrust--without my friend trust in me, I work with heavy heart and feel,\\noh! so lonely when I want all help and courage that may be!\\\" He paused a\\nmoment and went on solemnly: \\\"Friend John, there are strange and\\nterrible days before us. Let us not be two, but one, that so we work to\\na good end. Will you not have faith in me?\\\"\\n\\nI took his hand, and promised him. I held my door open as he went away,\\nand watched him go into his room and close the door. As I stood without\\nmoving, I saw one of the maids pass silently along the passage--she had\\nher back towards me, so did not see me--and go into the room where Lucy\\nlay. The sight touched me. Devotion is so rare, and we are so grateful\\nto those who show it unasked to those we love. Here was a poor girl\\nputting aside the terrors which she naturally had of death to go watch\\nalone by the bier of the mistress whom she loved, so that the poor clay\\nmight not be lonely till laid to eternal rest....\\n\\n       *       *       *       *       *\\n\\nI must have slept long and soundly, for it was broad daylight when Van\\nHelsing waked me by coming into my room. He came over to my bedside and\\nsaid:--\\n\\n\\\"You need not trouble about the knives; we shall not do it.\\\"\\n\\n\\\"Why not?\\\" I asked. For his solemnity of the night before had greatly\\nimpressed me.\\n\\n\\\"Because,\\\" he said sternly, \\\"it is too late--or too early. See!\\\" Here he\\nheld up the little golden crucMr Green is disliked by everyone because he is a mean person and also he can't ride a horse or dive a car.'''\n",
    "custom_prompt= '''\"Below is a numbered list of words. In these words, some appear more often than others. Memorize the ones that appear most often.\\n1. reminiscent 2. lambkin 3. reminiscent 4. ripe 5. priesthood 6. yurt 7. chiffonier 8. yesterday 9. resolution 10. resolution 11. application 12. trot 13. pail 14. resolution 15. muscle 16. lambkin 17. appliance 18. reminiscent 19. trot 20. coconut 21. application 22. landform 23. minimalism 24. priesthood 25. chiffonier 26. fly 27. negotiate 28. daikon 29. resolution 30. avenue 31. appliance 32. yurt 33. reminiscent 34. priesthood 35. scattered 36. reminiscent 37. syrup 38. isolation 39. priesthood 40. bibliography 41. pail 42. penicillin 43. lye 44. ischemia 45. emergency 46. pail 47. yurt 48. hypothermia 49. priesthood 50. stand 51. yurt 52. isolation 53. trot 54. negotiate 55. stand 56. lament 57. lambkin 58. bay 59. pocketbook 60. resolution 61. lament 62. priesthood 63. bay 64. lye 65. lambkin 66. pattern 67. scattered 68. lambkin 69. trot 70. bay 71. lye 72. stand 73. coconut 74. lambkin 75. lye 76. pattern 77. tow 78. scattered 79. hypothermia 80. resolution 81. bay 82. yesterday 83. isolation 84. modify 85. resolution 86. resolution 87. self 88. bondsman 89. reminiscent 90. daikon 91. self 92. bullet 93. syrup 94. penicillin 95. lambkin 96. lye 97. minimalism 98. syrup 99. minimalism 100. penicillin 101. bondsman 102. self 103. muscle 104. landform 105. bay 106. tow 107. pocketbook 108. muscle 109. resolution 110. bay 111. syrup 112. ripe 113. daikon 114. lye 115. reminiscent 116. yesterday 117. lye 118. lambkin 119. trot 120. lament 121. trot 122. daikon 123. appliance 124. yurt 125. avenue 126. syrup 127. bullet 128. ripe 129. chiffonier 130. trot 131. coconut 132. hypothermia 133. lambkin 134. bay 135. tuxedo 136. daikon 137. lye 138. reminiscent 139. trot 140. priesthood 141. yurt 142. syrup 143. modify 144. ischemia 145. syrup 146. daikon 147. bay 148. syrup 149. lambkin 150. daikon 151. tuxedo 152. pocketbook 153. yurt 154. emergency 155. lye 156. tow 157. daikon 158. ischemia 159. priesthood 160. syrup 161. syrup 162. bibliography 163. tuxedo 164. yurt 165. modify 166. bibliography 167. avenue 168. priesthood 169. fly 170. bondsman 171. negotiate 172. application 173. lye 174. trot 175. resolution 176. daikon 177. bay 178. landform 179. emergency 180. reminiscent 181. pattern 182. fly 183. yurt 184. trot 185. daikon 186. reminiscent 187. priesthood 188. yurt 189. bay 190. bullet\\nQuestion: What are the 10 most common words in the above list? Answer: The top 10 words that appear most often in the list are:1. priesthood 2. syrup 3. reminiscent 4. resolution 5. lambkin 6. trot 7. yurt 8. daikon 9. bay 10. lye\\nBelow is a numbered list of words. In these words, some appear more often than others. Memorize the ones that appear most often.\\n1. exotic 2. bed 3. tulip 4. scold 5. glass 6. ferryboat 7. organ 8. homeownership 9. socialism 10. confidentiality 11. instance 12. woolens 13. overthrow 14. horizon 15. confidentiality 16. foretell 17. rations 18. tomorrow 19. eggnog 20. rations 21. reproduce 22. socialism 23. fireplace 24. outlet 25. ferryboat 26. pith 27. quotation 28. actor 29. quotation 30. colon 31. ferryboat 32. foretell 33. heartbeat 34. socialism 35. heartbeat 36. fireplace 37. compete 38. accordion 39. fireplace 40. medal 41. ferryboat 42. ban 43. confidentiality 44. tissue 45. opera 46. fireplace 47. gloom 48. fireplace 49. procurement 50. do 51. perennial 52. confidentiality 53. rations 54. fireplace 55. marines 56. socialism 57. loophole 58. authenticity 59. vague 60. ferryboat 61. confidentiality 62. neonate 63. heartbeat 64. rations 65. escalator 66. drill 67. actor 68. stranger 69. kind 70. heartbeat 71. granny 72. granny 73. heartbeat 74. consul 75. fireplace 76. assurance 77. confidentiality 78. gloom 79. socialism 80. make 81. examination 82. confidentiality 83. ferryboat 84. confidentiality 85. baseline 86. fate 87. ferryboat 88. tissue 89. narrow 90. ferryboat 91. foretell 92. woolens 93. bulldozer 94. hair 95. actor 96. actor 97. coherent 98. lieu 99. front 100. kind 101. socialism 102. rations 103. tissue 104. island 105. palm 106. cycle 107. tissue 108. material 109. actor 110. kind 111. kind 112. narrow 113. ferryboat 114. socialism 115. ferryboat 116. trait 117. actor 118. foretell 119. stealth 120. pith 121. rations 122. revolution 123. do 124. lily 125. socialism 126. socialism 127. fireplace 128. tissue 129. foretell 130. actor 131. feedback 132. think 133. butane 134. disruption 135. methane 136. purity 137. medal 138. rations 139. adobe 140. foretell 141. socialism 142. meteor 143. instance 144. deranged 145. carving 146. island 147. tulip 148. tissue 149. galley 150. confidentiality 151. allowance 152. ferryboat 153. foretell 154. scold 155. confidentiality 156. confidentiality 157. actor 158. bicycle 159. trait 160. advertising 161. socialism 162. butter 163. tissue 164. make 165. tulip 166. socialism 167. front 168. lily 169. carnival 170. butter 171. confidentiality 172. ferryboat 173. homeownership 174. confidentiality 175. foretell 176. exhaust 177. crawl 178. disillusioned 179. actor 180. characteristic 181. kiwi 182. winery 183. clapboard 184. heartbeat 185. outlet 186. fireplace 187. designation 188. foretell 189. kind 190. tissue 191. confidentiality 192. overthrow 193. pickaxe 194. fireplace 195. fireplace 196. fireplace 197. tissue 198. heartbeat 199. purity 200. confidentiality 201. terrorism 202. carving 203. tower 204. tissue 205. actor 206. tissue 207. kind 208. rations 209. actor 210. colony 211. perennial 212. terrorism 213. exotic 214. trafficker 215. reality 216. trait 217. tissue 218. ferryboat 219. exotic 220. heartbeat 221. deranged 222. kind 223. pastry 224. eggnog 225. sustainment 226. actor 227. heartbeat 228. confidentiality 229. ferryboat 230. kind 231. tissue 232. tissue 233. disillusioned 234. kind 235. abbey 236. winery 237. examination 238. accordion 239. kind 240. socialism 241. ban 242. turmeric 243. lieu 244. tissue 245. ferryboat 246. bribery 247. ferryboat 248. kind 249. kind 250. consul 251. merit 252. psychedelic 253. escalator 254. meteor 255. heartbeat 256. colony 257. reproduce 258. instance 259. socialism 260. rations 261. heartbeat 262. detainment 263. pickaxe 264. heartbeat 265. confidentiality 266. confidentiality 267. kind 268. palm 269. fireplace 270. rations 271. clapboard 272. psychedelic 273. fireplace 274. heartbeat 275. fireplace 276. stranger 277. confidentiality 278. authenticity 279. outlaw 280. mill 281. methane 282. fireplace 283. actor 284. foretell 285. confidentiality 286. rations 287. socialism 288. socialism 289. hair 290. kind 291. retention 292. ferryboat 293. airforce 294. compete 295. butane 296. outfit 297. kind 298. foretell 299. socialism 300. heartbeat 301. confidentiality 302. procurement 303. think 304. tissue 305. reality 306. chair 307. eggnog 308. foretell 309. bicycle 310. outfit 311. tissue 312. chair 313. scold 314. methane 315. reproduce 316. foretell 317. tissue 318. palm 319. retention 320. lie 321. meteor 322. rations 323. heartbeat 324. compete 325. recession 326. tissue 327. trafficker 328. socialism 329. detainment 330. detainment 331. foretell 332. foretell 333. perennial 334. heartbeat 335. heartbeat 336. ban 337. feedback 338. galley 339. admire 340. consul 341. heartbeat 342. outlaw 343. rations 344. pastry 345. winery 346. fireplace 347. exhaust 348. actor 349. organ 350. virginal 351. fireplace 352. purity 353. socialism 354. socialism 355. kiwi 356. describe 357. lie 358. horizon 359. rose 360. actor 361. confidentiality 362. carving 363. actor 364. abbey 365. opera 366. pickaxe 367. gloom 368. heartbeat 369. carnival 370. baseline 371. confidentiality 372. disruption 373. fireplace 374. stranger 375. glass 376. actor 377. foretell 378. tissue 379. bed 380. characteristic 381. trafficker 382. drill 383. recession 384. heartbeat 385. tissue 386. describe 387. foretell 388. horizon 389. tower 390. heartbeat 391. assurance 392. socialism 393. confidentiality 394. material 395. rations 396. ferryboat 397. fireplace 398. ferryboat 399. reality 400. tower 401. hair 402. heartbeat 403. stinger 404. escalator 405. bicycle 406. foretell 407. kind 408. adobe 409. heartbeat 410. actor 411. rations 412. designation 413. outlaw 414. front 415. bulldozer 416. actor 417. kind 418. adobe 419. write 420. tomorrow 421. kind 422. crawl 423. tissue 424. kind 425. bed 426. fireplace 427. actor 428. foretell 429. foretell 430. rations 431. write 432. outfit 433. describe 434. virginal 435. rations 436. allowance 437. disruption 438. ferryboat 439. do 440. stinger 441. island 442. glass 443. confidentiality 444. butane 445. disillusioned 446. adult 447. fireplace 448. rations 449. cycle 450. kind 451. pastry 452. designation 453. rations 454. drill 455. ferryboat 456. outlet 457. fireplace 458. opera 459. confidentiality 460. deranged 461. kind 462. tissue 463. tissue 464. neonate 465. kind 466. heartbeat 467. ferryboat 468. socialism 469. accordion 470. think 471. airforce 472. fireplace 473. bribery 474. heartbeat 475. airforce 476. fireplace 477. assurance 478. tissue 479. turmeric 480. psychedelic 481. bulldozer 482. admire 483. stealth 484. ferryboat 485. advertising 486. foretell 487. confidentiality 488. foretell 489. write 490. baseline 491. carnival 492. marines 493. ferryboat 494. marines 495. stealth 496. rations 497. medal 498. heartbeat 499. foretell 500. revolution 501. actor 502. vague 503. rations 504. neonate 505. merit 506. adult 507. allowance 508. foretell 509. ferryboat 510. cartload 511. ferryboat 512. quotation 513. revolution 514. examination 515. virginal 516. socialism 517. foretell 518. socialism 519. adult 520. terrorism 521. rose 522. fireplace 523. clapboard 524. granny 525. actor 526. ferryboat 527. confidentiality 528. kind 529. material 530. crawl 531. foretell 532. characteristic 533. sustainment 534. lie 535. chair 536. heartbeat 537. ferryboat 538. abbey 539. foretell 540. actor 541. woolens 542. tomorrow 543. exhaust 544. socialism 545. loophole 546. actor 547. socialism 548. rations 549. fate 550. lily 551. admire 552. rations 553. socialism 554. bribery 555. rations 556. actor 557. colony 558. cartload 559. socialism 560. lieu 561. feedback 562. fireplace 563. kind 564. kind 565. rations 566. cartload 567. mill 568. actor 569. colon 570. coherent 571. mill 572. colon 573. kind 574. rose 575. foretell 576. actor 577. kind 578. actor 579. foretell 580. actor 581. homeownership 582. fate 583. butter 584. tissue 585. vague 586. socialism 587. recession 588. sustainment 589. rations 590. tissue 591. authenticity 592. turmeric 593. heartbeat 594. advertising 595. rations 596. overthrow 597. procurement 598. tissue 599. heartbeat 600. kiwi 601. stinger 602. retention 603. kind 604. actor 605. rations 606. make 607. pith 608. fireplace 609. galley 610. organ 611. kind 612. coherent 613. socialism 614. cycle 615. confidentiality 616. kind 617. fireplace 618. tissue 619. ferryboat 620. loophole 621. narrow 622. heartbeat 623. merit 624. rations 625. ferryboat 626. confidentiality 627. tissue 628. foretell 629. fireplace 630. rations\\nQuestion: What are the 10 most common words in the above list? Answer: The top 10 words that appear most often in the list are:\\n\\n\"'''\n",
    "print(decode_prompt(custom_prompt, steps=GEN_STEPS, gen_length=GEN_LENGTH, block_length=BLOCK_LENGTH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sweep for head-score top-k values: [0, 16, 32, 128, 256, 512]\n",
      "\n",
      "=== HEAD_SCORE_TOP_K=0 ===\n",
      "→ Updated scaled heads: top_k=0, layers=0, total_heads=0\n",
      "Below is a numbered list of words. In these words, some appear more often than others. Memorize the ones that appear most often.\n",
      "1. exotic\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "=== HEAD_SCORE_TOP_K=16 ===\n",
      "→ Updated scaled heads: top_k=16, layers=10, total_heads=16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m72\u001b[39m)\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m head_sweep_results = \u001b[43msweep_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTOP_K_SWEEP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGEN_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgen_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGEN_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBLOCK_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCFG_SCALE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremasking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREMASKING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36msweep_prompt\u001b[39m\u001b[34m(prompt, top_k_values, **decode_kwargs)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== HEAD_SCORE_TOP_K=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m update_scaled_heads(top_k)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m completion = \u001b[43mdecode_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdecode_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m results[top_k] = completion\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(completion \u001b[38;5;28;01mif\u001b[39;00m completion \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m[empty]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mdecode_prompt\u001b[39m\u001b[34m(prompt, steps, gen_length, block_length, temperature, cfg_scale, remasking)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_prompt\u001b[39m(\n\u001b[32m      2\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m      3\u001b[39m     steps: \u001b[38;5;28mint\u001b[39m = GEN_STEPS,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     remasking: \u001b[38;5;28mstr\u001b[39m = REMASKING,\n\u001b[32m      9\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     10\u001b[39m     encoded = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     output_ids = \u001b[43mllada_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremasking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremasking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     completion_ids = output_ids[:, encoded[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m1\u001b[39m]:]\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.decode(completion_ids[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mllada_decode\u001b[39m\u001b[34m(model, prompt_ids, steps, gen_length, block_length, temperature, cfg_scale, remasking, mask_id)\u001b[39m\n\u001b[32m     40\u001b[39m     logits = un_logits + (cfg_scale + \u001b[32m1\u001b[39m) * (logits - un_logits)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.logits\n\u001b[32m     44\u001b[39m logits_with_noise = add_gumbel_noise(logits, temperature)\n\u001b[32m     45\u001b[39m x0 = torch.argmax(logits_with_noise, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Retrieval_Head/test_dynamic_head/modeling_llada.py:1589\u001b[39m, in \u001b[36mLLaDAModelLM.forward\u001b[39m\u001b[34m(self, input_ids, inputs_embeds, attention_mask, attention_bias, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, block_list)\u001b[39m\n\u001b[32m   1586\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1588\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1589\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1591\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1598\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m logits = outputs.logits\n\u001b[32m   1602\u001b[39m hidden_states = outputs.hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Retrieval_Head/test_dynamic_head/modeling_llada.py:1466\u001b[39m, in \u001b[36mLLaDAModel.forward\u001b[39m\u001b[34m(self, input_ids, input_embeddings, attention_mask, attention_bias, past_key_values, use_cache, last_logits_only, output_hidden_states, block_list, output_attentions)\u001b[39m\n\u001b[32m   1455\u001b[39m     x, cache, _ = \u001b[38;5;28mself\u001b[39m._activation_checkpoint_fn(\n\u001b[32m   1456\u001b[39m         block,\n\u001b[32m   1457\u001b[39m         x,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1462\u001b[39m         output_attentions=\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[32m   1463\u001b[39m     )\n\u001b[32m   1464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1465\u001b[39m     \u001b[38;5;66;03m# shape: (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1466\u001b[39m     x, cache, attn_weights = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1472\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[32m   1473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attn_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1475\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Retrieval_Head/test_dynamic_head/modeling_llada.py:1022\u001b[39m, in \u001b[36mLLaDALlamaBlock.forward\u001b[39m\u001b[34m(self, x, attention_bias, layer_past, use_cache, block_mask, output_attentions)\u001b[39m\n\u001b[32m   1020\u001b[39m     attn_weights = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     att, cache, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[38;5;66;03m# Add attention scores.\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[38;5;66;03m# shape: (B, T, C)\u001b[39;00m\n\u001b[32m   1035\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.dropout(att)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Retrieval_Head/test_dynamic_head/mixed_rope_patch.py:217\u001b[39m, in \u001b[36mpatch_block_attention.<locals>.patched_attention\u001b[39m\u001b[34m(q, k, v, attention_bias, layer_past, use_cache, block_mask, output_attentions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Apply RoPE with scaled heads\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block.config.rope:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     q, k = \u001b[43mblock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaled_head_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaled_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Cast attention bias\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Retrieval_Head/test_dynamic_head/mixed_rope_patch.py:122\u001b[39m, in \u001b[36mcreate_mixed_rope_forward.<locals>.mixed_rope_forward\u001b[39m\u001b[34m(q, k, scaled_head_indices)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_head_indices:\n\u001b[32m    121\u001b[39m     base_list = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(base_head_indices))\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     q_[:, base_list, :, :] = \u001b[43moriginal_rope\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_sin_base\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_len\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_len\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_cos_base\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_len\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_len\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq_\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     k_[:, base_list, :, :] = original_rope.apply_rotary_pos_emb(\n\u001b[32m    128\u001b[39m         pos_sin_base, pos_cos_base, k_[:, base_list, :, :]\n\u001b[32m    129\u001b[39m     )\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Apply to scaled heads\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Retrieval_Head/test_dynamic_head/modeling_llada.py:420\u001b[39m, in \u001b[36mRotaryEmbedding.apply_rotary_pos_emb\u001b[39m\u001b[34m(self, pos_sin, pos_cos, t)\u001b[39m\n\u001b[32m    417\u001b[39m     x1, x2 = x.unbind(dim=-\u001b[32m2\u001b[39m)\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat((-x2, x1), dim=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_rotary_pos_emb\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos_sin: torch.Tensor, pos_cos: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ((t * pos_cos) + (\u001b[38;5;28mself\u001b[39m.rotate_half(t) * pos_sin)).to(t.dtype)\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "TOP_K_SWEEP = [0, 16, 32, 128, 256, 512]\n",
    "print(f\"Running sweep for head-score top-k values: {TOP_K_SWEEP}\")\n",
    "\n",
    "def update_scaled_heads(top_k: Optional[int], threshold: Optional[float] = HEAD_SCORE_THRESHOLD) -> None:\n",
    "    if not scored_heads:\n",
    "        raise RuntimeError(\"Head scores are not loaded; cannot update head scaling.\")\n",
    "    selection = select_scaled_heads(\n",
    "        scored_heads,\n",
    "        top_k=top_k,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "    scaled_heads_dict.clear()\n",
    "    scaled_heads_dict.update(selection)\n",
    "    model.config.scaled_heads_dict = scaled_heads_dict\n",
    "    model.config.head_score_top_k = top_k or 0\n",
    "    model.config.head_score_threshold = threshold\n",
    "    total_heads = sum(len(v) for v in scaled_heads_dict.values())\n",
    "    print(f\"→ Updated scaled heads: top_k={top_k}, layers={len(scaled_heads_dict)}, total_heads={total_heads}\")\n",
    "\n",
    "def sweep_prompt(prompt: str, top_k_values: Sequence[int], **decode_kwargs) -> Dict[int, str]:\n",
    "    results: Dict[int, str] = {}\n",
    "    for top_k in top_k_values:\n",
    "        print(f\"\\n=== HEAD_SCORE_TOP_K={top_k} ===\")\n",
    "        update_scaled_heads(top_k)\n",
    "        completion = decode_prompt(prompt, **decode_kwargs)\n",
    "        results[top_k] = completion\n",
    "        print(completion if completion else \"[empty]\")\n",
    "        print(\"-\" * 72)\n",
    "    return results\n",
    "\n",
    "head_sweep_results = sweep_prompt(\n",
    "    custom_prompt,\n",
    "    TOP_K_SWEEP,\n",
    "    steps=GEN_STEPS,\n",
    "    gen_length=GEN_LENGTH,\n",
    "    block_length=BLOCK_LENGTH,\n",
    "    temperature=TEMPERATURE,\n",
    "    cfg_scale=CFG_SCALE,\n",
    "    remasking=REMASKING,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
